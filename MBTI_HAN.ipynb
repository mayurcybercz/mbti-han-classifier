{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBTI HAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwjKOhOtyZ0x"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import nltk\n",
        "# nltk.download('word_tokenize')\n",
        "# nltk.download('sent_tokenize')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lKQGMv63Xks"
      },
      "source": [
        "# Data specific preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfdey25iyeF0"
      },
      "source": [
        "\n",
        "data = pd.read_csv('https://www.mustafaqazi.me/mbti_1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHQX1I0AykAQ",
        "outputId": "943a7ddd-c867-4aa7-db4d-aa39b9ad6bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>posts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>INFJ</td>\n",
              "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENTP</td>\n",
              "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>INTP</td>\n",
              "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>INTJ</td>\n",
              "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENTJ</td>\n",
              "      <td>'You're fired.|||That's another silly misconce...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type                                              posts\n",
              "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
              "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
              "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
              "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
              "4  ENTJ  'You're fired.|||That's another silly misconce..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cBHnLX3yl1s",
        "outputId": "a85ea875-3e0a-40ed-8541-6043222462ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "data['posts'][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"'I'm finding the lack of me in these posts very alarming.|||Sex can be boring if it's in the same position often. For example me and my girlfriend are currently in an environment where we have to creatively use cowgirl and missionary. There isn't enough...|||Giving new meaning to 'Game' theory.|||Hello *ENTP Grin*  That's all it takes. Than we converse and they do most of the flirting while I acknowledge their presence and return their words with smooth wordplay and more cheeky grins.|||This + Lack of Balance and Hand Eye Coordination.|||Real IQ test I score 127. Internet IQ tests are funny. I score 140s or higher.  Now, like the former responses of this thread I will mention that I don't believe in the IQ test. Before you banish...|||You know you're an ENTP when you vanish from a site for a year and a half, return, and find people are still commenting on your posts and liking your ideas/thoughts. You know you're an ENTP when you...|||http://img188.imageshack.us/img188/6422/6020d1f9da6944a6b71bbe6.jpg|||http://img.adultdvdtalk.com/813a0c6243814cab84c51|||I over think things sometimes. I go by the old Sherlock Holmes quote.  Perhaps, when a man has special knowledge and special powers like my  own, it rather encourages him to seek a complex...|||cheshirewolf.tumblr.com  So is I :D|||400,000+  post|||Not really; I've never thought of E/I or J/P as real functions.  I judge myself on what I use. I use Ne and Ti as my dominates. Fe for emotions and rarely Si. I also use Ni due to me strength...|||You know though. That was ingenious. After saying it I really want to try it and see what happens with me playing a first person shooter in the back while we drive around. I want to see the look on...|||out of all of them the rock paper one is the best. It makes me lol.  You guys are lucky :D I'm really high up on the tumblr system.|||So did you hear about that new first person shooter game? I've been rocking the hell out of the soundtrack on my auto sound equipment that will shake the heavens. We managed to put a couple PS3's in...|||No; The way he connected things was very Ne. Ne dominates are just as aware of their environments as Se dominates.  Example: Shawn Spencer or Patrick Jane; Both ENTPs.|||Well charlie I will be the first to admit I do get jealous like you do. I chalk it up to my 4w3 heart mixed with my dominate 7w8. 7s and 8s both like to be noticed. 4's like to be known (not the same...|||;D I'll upload the same clip with the mic away from my mouth. Than you won't hear anything.  Ninja Assassin style but with splatter.|||Tik Tok is a really great song. As long as you can mental block out the singer. I love the beat it makes me bounce.|||drop.io v1swck0  :D Mic really close to my mouth and smokin aces: assassins ball playing in the background.|||Sociable =/= extrovert; I'm an extrovert and I'm not sociable. :)|||Sherlock in the movie was an ENTP. Normally he's played as a EXTJ. In the books he's an ESTJ.  As I said. The movie looked good except for it being called sherlock holmes.|||http://i817.photobucket.com/albums/zz96/kamioo/Dirtywinch.png|||Oh, I never had fear of kissing a guy. I will kiss an animal too. So there was nothing to vanish. Just personal taste and me not liking it.  The guy I kissed didn't know me. It was one of those...|||Sounds pretty much like my area and what I'm going through right now trying to figure out which way I want to take my life. I want to do so many things. The biggest problem is that I know if I don't...|||;D I was operating under the impression that you were female. I never looked at your boxy. Okay, I help out my gay friends all the time and one of them has developed a little crush on me. I get red...|||T_T You just described me  and I'm living the worst nightmare. I'm trapped in one place with one one around. Only dull woods. If I was a serial killer this would be the perfect place but sadly I'm...|||TBH, and biased, sounds like a shadowed INFP. I think maybe he was hurt and turned ESTJ. I can tell because he has some of the typical INFP traits left over.|||*Checks list* I'm sorry. It seems that you have came at a bad time. We've already reached our quota of INFJs. However, being you're female and I like females I will make you a deal. I will kick one...|||I'm ANTP (Leaning toward E). I'm easy for both ENTPs and INTPs to identify with. :)|||I also imagine ENTP's interrogations would go a little bit like Jack's from 24 except more mechanical. Rigging up shock treatment equipment in an abandoned building out of an old car batty, jumper...|||It was a compliment :) Trust me. I'm just as psychopathic :D except I have emoticons. They're just weird ones. Like laughing when I get hurt or at people running themselves over with their lawn mower...|||http://i817.photobucket.com/albums/zz96/kamioo/Thunderstorm.pnghttp://i817.photobucket.com/albums/zz96/kamioo/Thunderstormbw.png http://i817.photobucket.com/albums/zz96/kamioo/Cosmicstorm.png|||No. It's like a theme for where I live and that is why I know it by heart.   http://www.youtube.com/watch?v=j5W73HaVQBg|||and I usual don't leave until the thing ends. But in the mean time. In between times. You work your thing. I'll work mine :D  ;D I'm the MBP; Pleasure to meet you.|||Damn, need to trust my instincts more I would have been closer I was going to say INFP.|||EXFP? Leaning toward S with the way she responded.  :D My friends, even my gay and lesbian ones, always come to me for advice.|||I bow to my entp masters ENTPs are so great. If it wasn't for ENTPs I wouldn't have been able to build what I'm building  Duck Duck  Duck  Shotgun|||What? Me? I never do that >.> <.<|||Because its hard to be sad about losing someone you like when you knew you were right and give yourself a big pat on the back because you're awesome and always correct.|||Oh, you don't have to tell me that most of them are stupid. I know this. That is why I play with them and it makes me laugh. :D As I'm going to take Neuropsychology and I have a few psychologist...|||:D I'm a Nightowl. I wake up between 6-7pm and stay awake till 10-11:30am.|||Personal opinion backed by theory would suggest that INTPs are the most socially difficult. While INTJs can be socially indifferent but they will also use social situations if the the need arises....|||Personal stocks that I have on my desktop that I've downloaded from random stock sites and stock photobuckets.|||I'll tell you when I open photoshop.  :) Glad you like it static.|||:D Thanks.|||http://i817.photobucket.com/albums/zz96/kamioo/Deathgrip.png http://i817.photobucket.com/albums/zz96/kamioo/Deathgripbw.png  Made for a friend. Several hours of work. I constructed every line by...|||:) Static: http://i817.photobucket.com/albums/zz96/kamioo/Statickitten.png  I'll have to get to your avatar later if one of my fellow teammates doesn't.|||Psychologist don't keep me around long enough to diagnosis me. I like to toy with them. What I have diagnosis myself with and had a few psychologist friends (+ a few other friends) tell me I have is...'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUGenbwynHP",
        "outputId": "4c3a4e47-fe8a-4d4f-cb87-3f3f536bccf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "data['type'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
              "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPzrOUrUyosj"
      },
      "source": [
        "def preprocess_story(data, remove_stop_words=True):\n",
        "  df = pd.DataFrame(columns=['sentence', 'category'])\n",
        "\n",
        "  LIMIT_SENTENCES = 23\n",
        "\n",
        "  # iterate over the dataset\n",
        "  for i in tqdm(range(len(data))):\n",
        "    # initalize one instance as empty\n",
        "    instance = \"\"\n",
        "    sentence_count = 0\n",
        "    row = data.iloc[i]\n",
        "    # split each sentence\n",
        "    sentences = row['posts'].split(\"|||\")\n",
        "    for sentence in sentences:\n",
        "      # check if sentence limit is reached\n",
        "      if sentence_count >= LIMIT_SENTENCES:\n",
        "        break\n",
        "      # Step 1: Remove links\n",
        "      sentence = re.sub(r'http\\S+', '', sentence)\n",
        "      # Step 2: Remove multiple full stops\n",
        "      sentence = re.sub(r'\\.+', \".\", sentence)\n",
        "      # Step 3: Some sentences do not have space after full stop, remove that\n",
        "      sentence = sentence.replace(\".\", \". \")\n",
        "      # Step 4: remove multiple spaces\n",
        "      sentence = re.sub(' +', ' ', sentence)\n",
        "      # Step 5: remove punctuations except . , ! & ? '\n",
        "      sentence = re.sub(\"[^a-zA-Z.,!&?']\", \" \", sentence)\n",
        "      # Step 6, if length of sentence is less than 5 dont add it\n",
        "      if len(sentence.split()) >= 5:\n",
        "        if instance != \"\":\n",
        "          instance = instance + \". \" + sentence\n",
        "        else:\n",
        "          instance = sentence\n",
        "        sentence_count += 1\n",
        "    # Step 7: Extra cleaning for instance\n",
        "    instance = instance.strip()\n",
        "    instance = re.sub(' +', ' ', instance)\n",
        "    instance = re.sub(r'\\.+', \".\", instance)\n",
        "    instance = instance.replace(\". .\", \".\")\n",
        "    instance = instance.replace(\" .\", \".\")\n",
        "    df.loc[len(df)] = [instance, row['type']]\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK10H_nm32vS",
        "outputId": "724794b9-b763-4b76-fd9e-9db66c91ac6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cleaned_data = preprocess_story(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8675/8675 [00:42<00:00, 202.12it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7cVMzz-3_k4",
        "outputId": "164f0199-4c10-484e-f579-5bbb332bf629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "cleaned_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>enfp and intj moments sportscenter not top ten...</td>\n",
              "      <td>INFJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
              "      <td>ENTP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Of course, to which I say I know that's my ble...</td>\n",
              "      <td>INTP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>'Dear INTP, I enjoyed our conversation the oth...</td>\n",
              "      <td>INTJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>That's another silly misconception. That appro...</td>\n",
              "      <td>ENTJ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence category\n",
              "0  enfp and intj moments sportscenter not top ten...     INFJ\n",
              "1  'I'm finding the lack of me in these posts ver...     ENTP\n",
              "2  Of course, to which I say I know that's my ble...     INTP\n",
              "3  'Dear INTP, I enjoyed our conversation the oth...     INTJ\n",
              "4  That's another silly misconception. That appro...     ENTJ"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZQG-vSK4Gyw",
        "outputId": "f87e0931-5a8a-4a46-8c5e-9840f84da358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "cleaned_data['sentence'][7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"'I tend to build up a collection of things on my desktop that i use frequently and then move them into a folder called 'Everything' from there it get sorted into type and sub type. i ike to collect odd objects, even at work. a lot of people would call it junk but i like to collect it. Old unused software? ill take that off your hands i have a bunch of old adobe. i think its quite normal, i tend to only see my friends in real life every couple of months, as said earlier some people just dont get it but the good ones do Edit i mostly mean tolerate it. where do we go when we sleep? is dreaming another form of being awake? how many more layers of this are there if any? thoughts about sleep keep me up at night Edit sometimes im too scared. i wish i was free to follow my interests as i desired i feel as though wishes are meant for impossible things. by seeing do you mean visual interpreting or seeing as in mentally understanding the concept?. i feel as though i am incapable of creating anything and i wish i could. i cant stand the interviewer christ that laugh. is he intj? hmmm it would be interesting to see an intj on this show, i doubt they would be that interesting to the general public though. know yourself and be yourself. Do you think Fi or Fe sounds more like me? which one do you think sounds like you? Why do you require input from others to know what you are?. Question do INTJs lean more towards Alternative Rock then other types of music? And if so, why? My Answer well, if you went through all the pages and then sorted all the songs by genre style. sometimes i look at people and i see them , well on the outside at least, doing all these things and saying all these words and i wonder what it would be like to act that way. am i missing out on. a lounge huh? what does one do in a lounge? or is it best not to define it and just enjoy it as is?. went on holiday for just over a month, thought things would change. How naive of me. feels nice to browse back on this forum here though, its been a while since i surrounded myself with somewhat. yes i would say i am, more than physical appearance to an certain degree. what are they? i am unsure they just generally cant be a terrible person by my standards. i like to lurk, in my case at least its mostly because i tend to believe i have nothing interesting to contribute to the conversation so why add anything? logging out to purposely lurk seems like. i think id wait for him to swing first before taking further action but i would not encourage them to take a swing a part of me wants to fight him though. would you say there is complexity in simplicity?. I normally vote for whoever amuses me the most. perhaps one day i'll care more Edit other than what amuses me at the time, ill vote for whatever would apparently benefit me the most. INTJ's and what effects their sanity levels Mental illness. So i think about which thoughts i wish to express in written format, then i proceed to perform the physical movements necessary to express the required thoughts in whichever medium is required or. i got this, seems right to me. I always score intj, extra heavy on the introversion. I like this I like the sound, the content lyrics are not too important, the sound is what i value most. Is it my favourite? probably not, i heard.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM8Lf5SQ5HRe"
      },
      "source": [
        "# HAN specific preprocessing\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EELlXlGP70ed"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import spacy\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.utils import tokenize\n",
        "from fastai.text import Tokenizer, Vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUsejZiD5GkA"
      },
      "source": [
        "# convert category to numerical\n",
        "categories = sorted(cleaned_data['category'].unique())\n",
        "\n",
        "def category_to_idx(cat):\n",
        "  return categories.index(cat)\n",
        "\n",
        "cleaned_data['category_class'] = cleaned_data.category.apply(category_to_idx).astype(\"int64\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpQIaXFk5-Qn",
        "outputId": "21eb8702-6587-4dd1-cec4-c859660041bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "cleaned_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>category</th>\n",
              "      <th>category_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>enfp and intj moments sportscenter not top ten...</td>\n",
              "      <td>INFJ</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
              "      <td>ENTP</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Of course, to which I say I know that's my ble...</td>\n",
              "      <td>INTP</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>'Dear INTP, I enjoyed our conversation the oth...</td>\n",
              "      <td>INTJ</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>That's another silly misconception. That appro...</td>\n",
              "      <td>ENTJ</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence category  category_class\n",
              "0  enfp and intj moments sportscenter not top ten...     INFJ               8\n",
              "1  'I'm finding the lack of me in these posts ver...     ENTP               3\n",
              "2  Of course, to which I say I know that's my ble...     INTP              11\n",
              "3  'Dear INTP, I enjoyed our conversation the oth...     INTJ              10\n",
              "4  That's another silly misconception. That appro...     ENTJ               2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR3C912x6SjV"
      },
      "source": [
        "texts = cleaned_data.sentence.tolist()\n",
        "target = cleaned_data.category_class.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwo-PUTW7kNA",
        "outputId": "5beafe65-d5ba-41e0-d493-fa8af1f26f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "texts[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"enfp and intj moments sportscenter not top ten plays pranks. What has been the most life changing experience in your life?. On repeat for most of today. May the PerC Experience immerse you. The last thing my INFJ friend posted on his facebook before committing suicide the next day. Rest in peace. Hello ENFJ. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as. Prozac, wellbrutin, at least thirty minutes of moving your legs and I don't mean moving them while sitting in your same desk chair , weed in moderation maybe try edibles as a healthier alternative. Basically come up with three items you've determined that each type or whichever types you want to do would more than likely use, given each types' cognitive functions and whatnot, when left by. All things in moderation. Sims is indeed a video game, and a good one at that. Note a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim. Dear ENFP What were your favorite video games growing up and what are your now, current favorite video games? cool. It appears to be too late. sad. There's someone out there for everyone. Wait. I thought confidence was a good thing. I just cherish the time of solitude b c i revel within my inner world more whereas most other time i'd be workin. just enjoy the me time while you can. Don't worry, people will always be around to. Yo entp ladies. if you're into a complimentary personality,well, hey.. when your main social outlet is xbox live conversations and even then you verbally fatigue quickly. I really dig the part from to. Banned because this thread requires it of me. Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses. Banned for too many b's in that sentence. How could you! Think of the B!. Banned for watching movies in the corner with the dunces. Banned because Health class clearly taught you nothing about peer pressure. Banned for a whole host of reasons!\",\n",
              " \"'I'm finding the lack of me in these posts very alarming. Sex can be boring if it's in the same position often. For example me and my girlfriend are currently in an environment where we have to creatively use cowgirl and missionary. There isn't enough. Giving new meaning to 'Game' theory. Hello ENTP Grin That's all it takes. Than we converse and they do most of the flirting while I acknowledge their presence and return their words with smooth wordplay and more cheeky grins. This Lack of Balance and Hand Eye Coordination. Real IQ test I score. Internet IQ tests are funny. I score s or higher. Now, like the former responses of this thread I will mention that I don't believe in the IQ test. Before you banish. You know you're an ENTP when you vanish from a site for a year and a half, return, and find people are still commenting on your posts and liking your ideas thoughts. You know you're an ENTP when you. I over think things sometimes. I go by the old Sherlock Holmes quote. Perhaps, when a man has special knowledge and special powers like my own, it rather encourages him to seek a complex. cheshirewolf. tumblr. com So is I D. Not really I've never thought of E I or J P as real functions. I judge myself on what I use. I use Ne and Ti as my dominates. Fe for emotions and rarely Si. I also use Ni due to me strength. You know though. That was ingenious. After saying it I really want to try it and see what happens with me playing a first person shooter in the back while we drive around. I want to see the look on. out of all of them the rock paper one is the best. It makes me lol. You guys are lucky D I'm really high up on the tumblr system. So did you hear about that new first person shooter game? I've been rocking the hell out of the soundtrack on my auto sound equipment that will shake the heavens. We managed to put a couple PS 's in. No The way he connected things was very Ne. Ne dominates are just as aware of their environments as Se dominates. Example Shawn Spencer or Patrick Jane Both ENTPs. Well charlie I will be the first to admit I do get jealous like you do. I chalk it up to my w heart mixed with my dominate w. s and s both like to be noticed. 's like to be known not the same. D I'll upload the same clip with the mic away from my mouth. Than you won't hear anything. Ninja Assassin style but with splatter. Tik Tok is a really great song. As long as you can mental block out the singer. I love the beat it makes me bounce. drop. io v swck D Mic really close to my mouth and smokin aces assassins ball playing in the background. Sociable extrovert I'm an extrovert and I'm not sociable. Sherlock in the movie was an ENTP. Normally he's played as a EXTJ. In the books he's an ESTJ. As I said. The movie looked good except for it being called sherlock holmes. Oh, I never had fear of kissing a guy. I will kiss an animal too. So there was nothing to vanish. Just personal taste and me not liking it. The guy I kissed didn't know me. It was one of those. Sounds pretty much like my area and what I'm going through right now trying to figure out which way I want to take my life. I want to do so many things. The biggest problem is that I know if I don't. D I was operating under the impression that you were female. I never looked at your boxy. Okay, I help out my gay friends all the time and one of them has developed a little crush on me. I get red.\",\n",
              " \"Of course, to which I say I know that's my blessing and my curse. Does being absolutely positive that you and your best friend could be an amazing couple count? If so, than yes. Or it's more I could be madly in love in case I reconciled my feelings which at. No, I didn't thank you for a link!. So called Ti Si loop and it can stem from any current topic obsession can be deadly. It's like when you're stuck in your own thoughts, and your mind just wanders in circles. Feels truly terrible.. Have you noticed how peculiar vegetation can be? All you have to do is look down at the grass dozens of different plant species there. And now imagine that hundreds of years later when if soil. The Smiths Never Had No One Ever. I often find myself spotting faces on marble tiles wood. This year old sentence is an incredibly accurate and beautiful description. I haven't visited this website in the last years. So whoever reads this and maybe even remembers me, which I highly doubt hi. When you sit in your garden until PM writing songs, and sing them together with dozens of crickets while playing your acoustic guitar. This is the most INTP ish thread I've ever seen. I wouldn't be able to look at the painting for the entire life if I knew that I picked it over the human being. I was drawing a background for my animation on which I'm working right now it should have been Mars. But I felt obligated to make Mark Watneyx s postcard from it D If you read the book. I started to make comics about turtle Gordon and unicorn Chimes here you can see two first stories. INTJ Recently I started to post my comics about two friends turtle Gordon and unicorn Chimes. Before that, I just posted stuff that interested me, but from now on I'll try to include only my works. Probably we could work together on a new model I'm an expert in abrupt explosions of laughter upon various weird stuff. That happens because of peculiar sense of humor so peculiar that not much. Hellooo Nah, you can touch it. Everyone thinks that it's scared or sad, but that's not true in fact it has an absolutely neutral face. And this kitten actually really likes patting and hugs only. Well. kind of As it was already mentioned, sometimes because of Ni it's hard to convey complex stuff which pops up in your head in whimsical compilations of shapes and pictures only with words. I think this kitten would be very appropriate here. GOOD NIGHT everyone out there! Even if for someone there is morning right now nights always supersede mornings. And people say good night in order to meet next day. Oh, that movie It's awesome Thank you! Hope you had good sleep in the air anyway, I'm wishing you good night for the next night ahead! hopefully it will be on land Good people deserve good. Well, other people who may be wondering about an issue from the name of the topic will find your response helpful anyway. This. Finally someone mentioned that\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXDGAAXk7mEa",
        "outputId": "e4770f24-7119-45ee-a93d-cdeed9c42b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "target[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 3, 11]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjI9ub7j7trm"
      },
      "source": [
        "tok_func = spacy.load(\"en_core_web_sm\")\n",
        "n_cpus = os.cpu_count()\n",
        "bsz = 100\n",
        "texts_sents = []\n",
        "for doc in tok_func.pipe(texts, n_process=n_cpus, batch_size=bsz):\n",
        "    sents = [str(s) for s in list(doc.sents)]\n",
        "    texts_sents.append(sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpHtuiEO7xYq",
        "outputId": "d68d44f3-bcc9-41c5-f01d-1f84dc4192ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "texts_sents[0] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['enfp and intj moments sportscenter',\n",
              " 'not top ten plays pranks.',\n",
              " 'What has been the most life changing experience in your life?.',\n",
              " 'On repeat for most of today.',\n",
              " 'May the PerC Experience immerse you.',\n",
              " 'The last thing my INFJ friend posted on his facebook before committing suicide the next day.',\n",
              " 'Rest in peace.',\n",
              " 'Hello ENFJ.',\n",
              " 'Sorry to hear of your distress.',\n",
              " \"It's only natural for a relationship to not be perfection all the time in every moment of existence.\",\n",
              " 'Try to figure the hard times as times of growth, as.',\n",
              " 'Prozac, wellbrutin, at least thirty minutes of moving your legs',\n",
              " \"and I don't mean moving them while sitting in your same desk chair ,\",\n",
              " 'weed in moderation maybe try edibles as a healthier alternative.',\n",
              " \"Basically come up with three items you've determined that each type or whichever types you want to do would more than likely use, given each types' cognitive functions and whatnot, when left by.\",\n",
              " 'All things in moderation.',\n",
              " 'Sims is indeed a video game, and a good one at that.',\n",
              " 'Note a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim.',\n",
              " 'Dear ENFP',\n",
              " 'What were your favorite video games growing up and what are your now, current favorite video games?',\n",
              " 'cool.',\n",
              " 'It appears to be too late.',\n",
              " 'sad.',\n",
              " \"There's someone out there for everyone.\",\n",
              " 'Wait.',\n",
              " 'I thought confidence was a good thing.',\n",
              " 'I just cherish the time of solitude b c',\n",
              " \"i revel within my inner world more whereas most other time i'd be workin.\",\n",
              " 'just enjoy the me time while you can.',\n",
              " \"Don't worry, people will always be around to.\",\n",
              " 'Yo entp ladies.',\n",
              " \"if you're into a complimentary personality,well, hey.. when your main social outlet is xbox live conversations and even then you verbally fatigue quickly.\",\n",
              " 'I really dig the part from to.',\n",
              " 'Banned because this thread requires it of me.',\n",
              " 'Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.',\n",
              " \"Banned for too many b's in that sentence.\",\n",
              " 'How could you!',\n",
              " 'Think of the B!.',\n",
              " 'Banned for watching movies in the corner with the dunces.',\n",
              " 'Banned because Health class clearly taught you nothing about peer pressure.',\n",
              " 'Banned for a whole host of reasons!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSaC6lKN79yF"
      },
      "source": [
        "# from nested to flat list.\n",
        "all_sents = [s for sents in texts_sents for s in sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHLNnKyd9C5N",
        "outputId": "8ff126ac-4f9a-4db1-9b71-c70b2e241d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "all_sents[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['enfp and intj moments sportscenter',\n",
              " 'not top ten plays pranks.',\n",
              " 'What has been the most life changing experience in your life?.',\n",
              " 'On repeat for most of today.',\n",
              " 'May the PerC Experience immerse you.',\n",
              " 'The last thing my INFJ friend posted on his facebook before committing suicide the next day.',\n",
              " 'Rest in peace.',\n",
              " 'Hello ENFJ.',\n",
              " 'Sorry to hear of your distress.',\n",
              " \"It's only natural for a relationship to not be perfection all the time in every moment of existence.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNujwxkW9EoN"
      },
      "source": [
        "# saving the lengths of the documents: 1) for padding purposes and 2) to compute consecutive ranges \n",
        "# so we can \"fold\" the list again\n",
        "texts_length = [0] + [len(s) for s in texts_sents]\n",
        "range_idx = [sum(texts_length[: i + 1]) for i in range(len(texts_length))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAArmtzl-xzj",
        "outputId": "6d12458b-60f7-4d4a-b6aa-b0b268378b76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "texts_length[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 41, 80, 52, 54, 57, 79, 73, 65, 55]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SszirjnQ_0hu",
        "outputId": "7dbad7ac-db00-4810-c7b8-a1ce5c72f4af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "range_idx[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 41, 121, 173, 227, 284, 363, 436, 501, 556]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVSUVF7YECqG"
      },
      "source": [
        "sents_tokens = Tokenizer().process_all(all_sents)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkf0uhYtEP00",
        "outputId": "5b0e305c-8bb3-4daa-c59c-2f2295b533d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sents_tokens[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['enfp', 'and', 'intj', 'moments', 'sportscenter']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-Wb4FlzEhBP"
      },
      "source": [
        "def simple_preprocess(doc, lower=False, deacc=False, min_len=2, max_len=16):\n",
        "    tokens = [\n",
        "        token\n",
        "        for token in tokenize(doc, lower=False, deacc=deacc, errors=\"ignore\")\n",
        "        if min_len <= len(token) <= max_len and not token.startswith(\"_\")\n",
        "    ]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def get_texts(texts, with_preprocess=False):\n",
        "    if with_preprocess:\n",
        "        texts = [\" \".join(simple_preprocess(s)) for s in texts]\n",
        "    tokens = Tokenizer().process_all(texts)\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfUsrSrFFDlP"
      },
      "source": [
        "sents_tokens_2 = get_texts(all_sents, with_preprocess=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "439-P4pwFHZ7",
        "outputId": "88a072e0-b589-41b1-c043-2f7ba353d345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sents_tokens_2[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['enfp', 'and', 'intj', 'moments', 'sportscenter']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6MYo96bFSxx"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9V__2swFUHq"
      },
      "source": [
        "s = \"I don't: particularly ; like data science.\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TATpdBEOFXvW",
        "outputId": "244456e8-f9e5-40f6-8fb0-6f1cbfd99b1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_texts([s], with_preprocess=False)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'do', \"n't\", ':', 'particularly', ';', 'like', 'data', 'science', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ_ZHhUcFaGW",
        "outputId": "3a16d298-0c06-4bc4-c59d-e5d92557e559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_texts([s], with_preprocess=True)[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['don', 'particularly', 'like', 'data', 'science']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImGVg8GTFhQe"
      },
      "source": [
        "### End example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98e_YwlAFbcL"
      },
      "source": [
        "import re\n",
        "def rm_punctuation(x):\n",
        "    x = re.sub(r\"\\.|,|:|;\", \" \", x)\n",
        "    # or \n",
        "    # x = x.replace(\".\", \"\").replace(\",\",\"\").replace(\":\", \"\").replace(\";\",\"\")\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2-ITdryFn6L"
      },
      "source": [
        "new_tok = Tokenizer()\n",
        "new_tok.pre_rules = [rm_punctuation] + new_tok.pre_rules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIHJXuvUFpp-",
        "outputId": "72b61346-ad15-4eba-d838-adaf43129c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_tok.process_all([s])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['i', 'do', \"n't\", 'particularly', 'like', 'data', 'science']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmFE544mFvGe"
      },
      "source": [
        "#  saving the lengths of sentences for padding purposes\n",
        "sents_length = [len(s) for s in sents_tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZxWwP8OFx3Z",
        "outputId": "ba3bdc08-f15b-4dd4-9949-864db433bf9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sents_length[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 6, 14, 8, 9]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_wry6u3F0GQ"
      },
      "source": [
        "# Create Vocabulary using fastai's Vocab class\n",
        "vocab = Vocab.create(sents_tokens, max_vocab=60000, min_freq=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBzJ5iM8F6-N"
      },
      "source": [
        "# 'numericalize' each sentence\n",
        "sents_numz = [vocab.numericalize(s) for s in sents_tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoqrPBUzGMAS",
        "outputId": "fd8d3c79-ee69-48ac-f07b-e14ac02c5fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "sents_numz[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[179, 14, 146, 1283, 0],\n",
              " [35, 686, 1873, 2093, 8979, 9],\n",
              " [5, 46, 105, 86, 12, 101, 115, 1441, 302, 21, 51, 115, 36, 9],\n",
              " [5, 38, 2240, 27, 101, 17, 366, 9],\n",
              " [5, 209, 12, 846, 5, 302, 11374, 18, 9]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWRNkyFPHoyc"
      },
      "source": [
        "# group the sentences again into documents\n",
        "texts_numz = [sents_numz[range_idx[i] : range_idx[i + 1]] for i in range(len(range_idx[:-1]))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTpq2Z-2HqWx",
        "outputId": "20b3e392-b2a6-48f2-f400-7c1bab13befe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "texts_numz[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[179, 14, 146, 1283, 0],\n",
              " [35, 686, 1873, 2093, 8979, 9],\n",
              " [5, 46, 105, 86, 12, 101, 115, 1441, 302, 21, 51, 115, 36, 9],\n",
              " [5, 38, 2240, 27, 101, 17, 366, 9],\n",
              " [5, 209, 12, 846, 5, 302, 11374, 18, 9],\n",
              " [5,\n",
              "  12,\n",
              "  255,\n",
              "  126,\n",
              "  22,\n",
              "  6,\n",
              "  142,\n",
              "  162,\n",
              "  581,\n",
              "  38,\n",
              "  151,\n",
              "  1046,\n",
              "  196,\n",
              "  5985,\n",
              "  2228,\n",
              "  12,\n",
              "  473,\n",
              "  194,\n",
              "  9],\n",
              " [5, 786, 21, 1321, 9],\n",
              " [5, 511, 6, 336, 9],\n",
              " [5, 292, 13, 435, 17, 51, 7393, 9],\n",
              " [5,\n",
              "  16,\n",
              "  24,\n",
              "  102,\n",
              "  825,\n",
              "  27,\n",
              "  15,\n",
              "  247,\n",
              "  13,\n",
              "  35,\n",
              "  34,\n",
              "  4213,\n",
              "  56,\n",
              "  12,\n",
              "  73,\n",
              "  21,\n",
              "  221,\n",
              "  482,\n",
              "  17,\n",
              "  1445,\n",
              "  9],\n",
              " [5, 171, 13, 551, 12, 218, 261, 41, 261, 17, 2487, 11, 41, 9],\n",
              " [5, 10934, 11, 0, 11, 55, 273, 6469, 1031, 17, 1081, 51, 3322],\n",
              " [14, 10, 25, 23, 177, 1081, 78, 178, 1082, 21, 51, 148, 2987, 3816, 11],\n",
              " [2101, 21, 6845, 166, 171, 19338, 41, 15, 5799, 2196, 9],\n",
              " [5,\n",
              "  528,\n",
              "  228,\n",
              "  75,\n",
              "  29,\n",
              "  553,\n",
              "  3379,\n",
              "  18,\n",
              "  64,\n",
              "  2784,\n",
              "  19,\n",
              "  334,\n",
              "  109,\n",
              "  47,\n",
              "  6846,\n",
              "  235,\n",
              "  18,\n",
              "  98,\n",
              "  13,\n",
              "  25,\n",
              "  57,\n",
              "  61,\n",
              "  110,\n",
              "  427,\n",
              "  232,\n",
              "  11,\n",
              "  645,\n",
              "  334,\n",
              "  235,\n",
              "  65,\n",
              "  649,\n",
              "  351,\n",
              "  14,\n",
              "  4089,\n",
              "  11,\n",
              "  52,\n",
              "  517,\n",
              "  87,\n",
              "  9],\n",
              " [5, 56, 90, 21, 6845, 9],\n",
              " [5, 5800, 20, 1098, 15, 477, 484, 11, 14, 15, 93, 59, 55, 19, 9],\n",
              " [5,\n",
              "  937,\n",
              "  15,\n",
              "  93,\n",
              "  59,\n",
              "  55,\n",
              "  19,\n",
              "  20,\n",
              "  877,\n",
              "  1423,\n",
              "  21,\n",
              "  19,\n",
              "  10,\n",
              "  72,\n",
              "  35,\n",
              "  374,\n",
              "  9837,\n",
              "  12,\n",
              "  706,\n",
              "  17,\n",
              "  124,\n",
              "  645,\n",
              "  5,\n",
              "  10542,\n",
              "  9],\n",
              " [5, 577, 6, 179],\n",
              " [5,\n",
              "  46,\n",
              "  143,\n",
              "  51,\n",
              "  432,\n",
              "  477,\n",
              "  582,\n",
              "  1099,\n",
              "  75,\n",
              "  14,\n",
              "  46,\n",
              "  33,\n",
              "  51,\n",
              "  108,\n",
              "  11,\n",
              "  912,\n",
              "  432,\n",
              "  477,\n",
              "  582,\n",
              "  36],\n",
              " [379, 9],\n",
              " [5, 16, 1949, 13, 34, 85, 693, 9],\n",
              " [490, 9],\n",
              " [5, 66, 24, 117, 68, 66, 27, 206, 9],\n",
              " [5, 678, 9],\n",
              " [10, 153, 1116, 39, 15, 93, 126, 9],\n",
              " [10, 45, 7248, 12, 73, 17, 3617, 948, 1218],\n",
              " [10, 14457, 833, 22, 1199, 198, 61, 2611, 101, 91, 73, 10, 130, 34, 0, 9],\n",
              " [45, 322, 12, 30, 73, 178, 18, 53, 9],\n",
              " [5, 25, 23, 819, 11, 50, 99, 113, 34, 167, 13, 9],\n",
              " [5, 2174, 207, 2679, 9],\n",
              " [44,\n",
              "  18,\n",
              "  80,\n",
              "  127,\n",
              "  15,\n",
              "  10543,\n",
              "  260,\n",
              "  11,\n",
              "  83,\n",
              "  11,\n",
              "  386,\n",
              "  190,\n",
              "  52,\n",
              "  51,\n",
              "  755,\n",
              "  311,\n",
              "  5478,\n",
              "  20,\n",
              "  7545,\n",
              "  318,\n",
              "  967,\n",
              "  14,\n",
              "  112,\n",
              "  104,\n",
              "  18,\n",
              "  3495,\n",
              "  9515,\n",
              "  988,\n",
              "  9],\n",
              " [10, 62, 3214, 12, 263, 69, 13, 9],\n",
              " [5, 2595, 70, 31, 165, 2149, 16, 17, 30, 9],\n",
              " [5,\n",
              "  71,\n",
              "  281,\n",
              "  21,\n",
              "  7719,\n",
              "  11,\n",
              "  9211,\n",
              "  14,\n",
              "  660,\n",
              "  0,\n",
              "  21,\n",
              "  7719,\n",
              "  178,\n",
              "  7896,\n",
              "  155,\n",
              "  95,\n",
              "  1334,\n",
              "  11,\n",
              "  2047,\n",
              "  87,\n",
              "  17752,\n",
              "  14,\n",
              "  5720,\n",
              "  9],\n",
              " [5, 2595, 27, 85, 156, 948, 24, 21, 19, 1645, 9],\n",
              " [5, 67, 107, 18, 42],\n",
              " [5, 49, 17, 12, 948, 42, 9],\n",
              " [5, 2595, 27, 544, 709, 21, 12, 2175, 29, 12, 0, 9],\n",
              " [5, 2595, 70, 5, 1244, 594, 945, 1719, 18, 286, 48, 5884, 1703, 9],\n",
              " [5, 2595, 27, 15, 355, 5058, 17, 676, 42]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GahCg_eMHvNt",
        "outputId": "011a8015-6de6-48d4-a08f-447a071cff5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(sents_tokens[1])\n",
        "print([vocab.itos[i] for i in texts_numz[0][1]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['not', 'top', 'ten', 'plays', 'pranks', '.']\n",
            "['not', 'top', 'ten', 'plays', 'pranks', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPrHoF74H-Ll"
      },
      "source": [
        "q=0.8\n",
        "maxlen_sent = int(np.quantile(sents_length, q=q))\n",
        "maxlen_doc  = int(np.quantile(texts_length[1:], q=q))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5KU88d_IAnt",
        "outputId": "e8c06d09-a94e-42f4-ae4c-357e50d53bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(maxlen_sent, maxlen_doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18 79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVbGdfCdICxh"
      },
      "source": [
        "def pad_sequences(seq, maxlen, pad_first=True, pad_idx=1):\n",
        "    if len(seq) >= maxlen:\n",
        "        res = np.array(seq[-maxlen:]).astype(\"int32\")\n",
        "        return res\n",
        "    else:\n",
        "        res = np.zeros(maxlen, dtype=\"int32\") + pad_idx\n",
        "        if pad_first:\n",
        "            res[-len(seq) :] = seq\n",
        "        else:\n",
        "            res[: len(seq) :] = seq\n",
        "        return res\n",
        "\n",
        "\n",
        "def pad_nested_sequences(\n",
        "    seq, maxlen_sent, maxlen_doc, pad_sent_first=True, pad_doc_first=False, pad_idx=1\n",
        "):\n",
        "    seq = [s for s in seq if len(s) >= 1]\n",
        "    if len(seq) == 0:\n",
        "        return np.array([[pad_idx] * maxlen_sent] * maxlen_doc).astype(\"int32\")\n",
        "    seq = [pad_sequences(s, maxlen_sent, pad_sent_first, pad_idx) for s in seq]\n",
        "    if len(seq) >= maxlen_doc:\n",
        "        return np.array(seq[:maxlen_doc])\n",
        "    else:\n",
        "        res = np.array([[pad_idx] * maxlen_sent] * maxlen_doc).astype(\"int32\")\n",
        "        if pad_doc_first:\n",
        "            res[-len(seq) :] = seq\n",
        "        else:\n",
        "            res[: len(seq) :] = seq\n",
        "        return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03xYjqwFINEx"
      },
      "source": [
        "padded_texts = np.stack([pad_nested_sequences(r, maxlen_sent, maxlen_doc) for r in texts_numz], axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32QeW9WcIPGY",
        "outputId": "06dfcc38-6f28-41ee-c847-32b247cc1e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "padded_texts.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8675, 79, 18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2EjnhplIQ5s",
        "outputId": "36516444-684a-4dd1-d90b-c02e0de3d1e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "padded_texts[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   1,    1,    1,    1, ...,   14,  146, 1283,    0],\n",
              "       [   1,    1,    1,    1, ..., 1873, 2093, 8979,    9],\n",
              "       [   1,    1,    1,    1, ...,   51,  115,   36,    9],\n",
              "       [   1,    1,    1,    1, ...,  101,   17,  366,    9],\n",
              "       ...,\n",
              "       [   1,    1,    1,    1, ...,    1,    1,    1,    1],\n",
              "       [   1,    1,    1,    1, ...,    1,    1,    1,    1],\n",
              "       [   1,    1,    1,    1, ...,    1,    1,    1,    1],\n",
              "       [   1,    1,    1,    1, ...,    1,    1,    1,    1]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ8J_VR4IVnn"
      },
      "source": [
        "# HAN implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIRNFDPWITRI"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kGGbiy4IagU"
      },
      "source": [
        "class AttentionWithContext(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(AttentionWithContext, self).__init__()\n",
        "\n",
        "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # The first expression in the attention mechanism is simply a linear layer that receives \n",
        "        # the output of the Word-GRU referred here as 'inp' and h_{it} in the paper\n",
        "        u = torch.tanh_(self.attn(inp))\n",
        "        # The second expression is...the same but without bias, wrapped up in a Softmax function\n",
        "        a = F.softmax(self.contx(u), dim=1)\n",
        "        # And finally, an element-wise multiplication taking advantage of Pytorch's broadcasting abilities \n",
        "        s = (a * inp).sum(1)\n",
        "        # we will also return the normalized importance weights\n",
        "        return a.permute(0, 2, 1), s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WJdS0YmIcQI"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim, seq_len):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.weight = nn.Parameter(nn.init.kaiming_normal_(torch.Tensor(hidden_dim, 1)))\n",
        "        self.bias = nn.Parameter(torch.zeros(seq_len))\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # 1. Matrix Multiplication\n",
        "        x = inp.contiguous().view(-1, self.hidden_dim)\n",
        "        u = torch.tanh_(torch.mm(x, self.weight).view(-1, self.seq_len) + self.bias)\n",
        "        # 2. Softmax on 'u_{it}' directly\n",
        "        a = F.softmax(u, dim=1)\n",
        "        # 3. Braodcasting and out\n",
        "        s = (inp * torch.unsqueeze(a, 2)).sum(1)\n",
        "        return a, s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4bSUcPtIdvW"
      },
      "source": [
        "class WordAttnNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        hidden_dim=32,\n",
        "        padding_idx=1,\n",
        "        embed_dim=50,\n",
        "        embedding_matrix=None,\n",
        "    ):\n",
        "        super(WordAttnNet, self).__init__()\n",
        "\n",
        "        if isinstance(embedding_matrix, np.ndarray):\n",
        "            self.word_embed = nn.Embedding(\n",
        "                vocab_size, embedding_matrix.shape[1], padding_idx=padding_idx\n",
        "            )\n",
        "            self.word_embed.weight = nn.Parameter(torch.Tensor(embedding_matrix))\n",
        "            embed_dim = embedding_matrix.shape[1]\n",
        "        else:\n",
        "            self.word_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "\n",
        "        self.rnn = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.word_attn = AttentionWithContext(hidden_dim * 2)\n",
        "\n",
        "    def forward(self, X, h_n):\n",
        "        embed = self.word_embed(X.long())\n",
        "        h_t, h_n = self.rnn(embed, h_n)\n",
        "        a, s = self.word_attn(h_t)\n",
        "        return a, s.unsqueeze(1), h_n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x46wRJaOIg-3"
      },
      "source": [
        "bsz = 16\n",
        "maxlen_sent = maxlen_sent # 20 \n",
        "hidden_dim  = 32    \n",
        "embed_dim   = 100    \n",
        "vocab_size  = len(vocab.stoi)\n",
        "padding_idx = 1\n",
        "\n",
        "# net\n",
        "word_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "rnn = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "attn = nn.Linear(hidden_dim*2, hidden_dim*2)\n",
        "contx = nn.Linear(hidden_dim*2, 1, bias=False)\n",
        "\n",
        "# inputs\n",
        "X = torch.from_numpy(np.random.choice(vocab_size, (bsz, maxlen_sent)))\n",
        "h_n = torch.zeros((2, bsz, hidden_dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo2U9PIZIxn7",
        "outputId": "c6d1912e-5039-41bd-b560-d6b520dc7076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 1. Word Embeddings\n",
        "# (bsz, maxlen_sent, embed_dim)\n",
        "embed = word_embed(X)\n",
        "embed.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 18, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75OpfWtVI0FH",
        "outputId": "2511bf8a-2d73-417f-9085-b24765d85303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# 2. GRU\n",
        "h_t, h_n = rnn(embed, h_n)\n",
        "# (bsz, seq_len, hidden_dim*2)\n",
        "h_t.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 18, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9OnswQ5I14b",
        "outputId": "453540f8-722b-454c-cb32-7a50978e1f6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# 3. Attention\n",
        "u = torch.tanh_(attn(h_t))\n",
        "a = F.softmax(contx(u), dim=1)\n",
        "print(h_t.shape, a.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 18, 64]) torch.Size([16, 18, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiEDfuG-I3X7",
        "outputId": "e1767674-ac18-4eaf-c4b3-ae31f3bf4280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# RNN outputs scaled by their importance weights\n",
        "s = (a * h_t)\n",
        "print(s.shape)\n",
        "# Sum along the seq dim so we end up with a representation per document/review\n",
        "s = s.sum(1)\n",
        "print(s.shape)\n",
        "# Because this will be stack for all sentences, we do the `.unsqueeze(1)`\n",
        "print(s.unsqueeze(1).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 18, 64])\n",
            "torch.Size([16, 64])\n",
            "torch.Size([16, 1, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q83ILVsHI5JA"
      },
      "source": [
        "class SentAttnNet(nn.Module):\n",
        "    def __init__(\n",
        "        self, word_hidden_dim=32, sent_hidden_dim=32, padding_idx=1\n",
        "    ):\n",
        "        super(SentAttnNet, self).__init__()\n",
        "\n",
        "        self.rnn = nn.GRU(\n",
        "            word_hidden_dim * 2, sent_hidden_dim, bidirectional=True, batch_first=True\n",
        "        )\n",
        "\n",
        "        self.sent_attn = AttentionWithContext(sent_hidden_dim * 2)\n",
        "\n",
        "    def forward(self, X):\n",
        "        h_t, h_n = self.rnn(X)\n",
        "        a, v = self.sent_attn(h_t)\n",
        "        return a.permute(0,2,1), v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKU3y_QpI6rM"
      },
      "source": [
        "class HierAttnNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        maxlen_sent,\n",
        "        maxlen_doc,\n",
        "        word_hidden_dim=32,\n",
        "        sent_hidden_dim=32,\n",
        "        padding_idx=1,\n",
        "        embed_dim=50,\n",
        "        embedding_matrix=None,\n",
        "        num_class=4,\n",
        "    ):\n",
        "        super(HierAttnNet, self).__init__()\n",
        "\n",
        "        self.word_hidden_dim = word_hidden_dim\n",
        "\n",
        "        self.wordattnnet = WordAttnNet(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_dim=word_hidden_dim,\n",
        "            padding_idx=padding_idx,\n",
        "            embed_dim=embed_dim,\n",
        "            embedding_matrix=embedding_matrix,\n",
        "        )\n",
        "\n",
        "        self.sentattnnet = SentAttnNet(\n",
        "            word_hidden_dim=word_hidden_dim,\n",
        "            sent_hidden_dim=sent_hidden_dim,\n",
        "            padding_idx=padding_idx,\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(sent_hidden_dim * 2, num_class)\n",
        "\n",
        "    def forward(self, X):\n",
        "        x = X.permute(1, 0, 2)\n",
        "        word_h_n = nn.init.zeros_(torch.Tensor(2, X.shape[0], self.word_hidden_dim))\n",
        "        if use_cuda:\n",
        "            word_h_n = word_h_n.cuda()\n",
        "        # alpha and s Tensor Lists\n",
        "        word_a_list, word_s_list = [], []\n",
        "        for sent in x:\n",
        "            word_a, word_s, word_h_n = self.wordattnnet(sent, word_h_n)\n",
        "            word_a_list.append(word_a)\n",
        "            word_s_list.append(word_s)\n",
        "        # Importance attention weights per word in sentence\n",
        "        self.sent_a = torch.cat(word_a_list, 1)\n",
        "        # Sentences representation\n",
        "        sent_s = torch.cat(word_s_list, 1)\n",
        "        # Importance attention weights per sentence in doc and document representation\n",
        "        self.doc_a, doc_s = self.sentattnnet(sent_s)\n",
        "        return self.fc(doc_s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBCRZZAYI-is"
      },
      "source": [
        "maxlen_sent = maxlen_sent # 20\n",
        "maxlen_doc =  maxlen_doc # 5\n",
        "num_class = len(cleaned_data['category'].unique()) # 4\n",
        "word_hidden_dim = 32\n",
        "sent_hidden_dim = 32\n",
        "\n",
        "wordattnnet = WordAttnNet(vocab_size, hidden_dim, padding_idx, embed_dim, embedding_matrix=None)\n",
        "sentattnnet = SentAttnNet(word_hidden_dim, sent_hidden_dim, padding_idx)\n",
        "fc = nn.Linear(sent_hidden_dim * 2, num_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-ssYyL1JSFn"
      },
      "source": [
        "X = torch.from_numpy(np.random.choice(vocab_size, (bsz, maxlen_doc, maxlen_sent)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ9MpLLTJbqH",
        "outputId": "5dbc4146-9b22-42e8-9fa1-95f60d14777d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = X.permute(1, 0, 2)\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([79, 16, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z3b3bNHJf4w"
      },
      "source": [
        "# Initial Word RNN hidden state\n",
        "word_h_n = nn.init.zeros_(torch.Tensor(2, X.shape[0], word_hidden_dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDXGcSQCJhq-",
        "outputId": "d7e42235-b92b-4a14-fb7b-b701faa0facf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Loop through sentences:\n",
        "word_a_list, word_s_list = [], []\n",
        "for sent in x:\n",
        "    word_a, word_s, word_h_n = wordattnnet(sent, word_h_n)\n",
        "    word_a_list.append(word_a)\n",
        "    word_s_list.append(word_s)\n",
        "# Importance attention weights per word in sentence\n",
        "sent_a = torch.cat(word_a_list, 1)\n",
        "# Sentences representation\n",
        "sent_s = torch.cat(word_s_list, 1)\n",
        "# (bsz, maxlen_doc, maxlen_sent)\n",
        "print(sent_a.shape)\n",
        "# (bsz, maxlen_doc, hidden_dim*2)\n",
        "print(sent_s.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 79, 18])\n",
            "torch.Size([16, 79, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3RX3_fLJi90",
        "outputId": "653d4867-2d87-463c-cadd-49a3d87d3fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "doc_a, doc_s = sentattnnet(sent_s)\n",
        "# (bsz, maxlen_doc, 1). One could .squeeze(2)\n",
        "print(doc_a.shape)\n",
        "# (bsz, hidden_dim*2)\n",
        "print(doc_s.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 79, 1])\n",
            "torch.Size([16, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW7t4Rn0JkUn",
        "outputId": "01edc8e7-c1f7-4333-8e7a-bc777189704e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "out = fc(doc_s)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 16])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkTluwNCJrf5"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_xw-uwhJliL"
      },
      "source": [
        "class CategoricalAccuracy(object):\n",
        "    def __init__(self, top_k=1):\n",
        "        self.top_k = top_k\n",
        "        self.correct_count = 0\n",
        "        self.total_count = 0\n",
        "\n",
        "        self._name = \"acc\"\n",
        "\n",
        "    def reset(self):\n",
        "        self.correct_count = 0\n",
        "        self.total_count = 0\n",
        "\n",
        "    def __call__(self, y_pred, y_true):\n",
        "        top_k = y_pred.topk(self.top_k, 1)[1]\n",
        "        true_k = y_true.view(len(y_true), 1).expand_as(top_k)\n",
        "        self.correct_count += top_k.eq(true_k).float().sum().item()\n",
        "        self.total_count += len(y_pred)\n",
        "        accuracy = float(self.correct_count) / float(self.total_count)\n",
        "        return np.round(accuracy, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2m6q4cmJyAr"
      },
      "source": [
        "# Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYvEi0mTJxgq"
      },
      "source": [
        " X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "        padded_texts, cleaned_data.category_class, train_size=0.8, random_state=1, stratify=cleaned_data.category_class\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHQjME5wKOZF"
      },
      "source": [
        "X_valid, X_test, y_valid, y_test = train_test_split(\n",
        "        X_valid, y_valid, train_size=0.5, random_state=1, stratify=y_valid\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHSVAikZLAwx"
      },
      "source": [
        "# Running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWJj69ASKA8H"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import trange\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
        "from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "# from utils.metrics import CategoricalAccuracy\n",
        "# from utils.parser import parse_args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iFgkKNpLD71"
      },
      "source": [
        "n_cpus = os.cpu_count()\n",
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzAhr3VbLFgL"
      },
      "source": [
        "# this wont be used\n",
        "\n",
        "data_dir = Path(\"data\")\n",
        "train_dir = data_dir / \"train\"\n",
        "valid_dir = data_dir / \"valid\"\n",
        "test_dir = data_dir / \"test\"\n",
        "\n",
        "ftrain, fvalid, ftest = \"han_train.npz\", \"han_valid.npz\", \"han_test.npz\"\n",
        "tokf = \"HANPreprocessor.p\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdWjeT3JLKgD"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_set = TensorDataset(\n",
        "    torch.from_numpy(np.array(X_train)),\n",
        "    torch.from_numpy(np.array(y_train)).long(),\n",
        ")\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, num_workers=n_cpus)\n",
        "\n",
        "\n",
        "np.random.seed(2)\n",
        "\n",
        "eval_set = TensorDataset(\n",
        "    torch.from_numpy(np.array(X_valid)),\n",
        "    torch.from_numpy(np.array(y_valid)).long(),\n",
        ")\n",
        "eval_loader = DataLoader(\n",
        "    dataset=eval_set, batch_size=batch_size, num_workers=n_cpus, shuffle=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaSx-X5eLMQd",
        "outputId": "ba8ed5e8-93a9-445f-a90b-59a25a6f8bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        }
      },
      "source": [
        "next(iter(train_loader))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[[   1,    1,    1,  ...,    5,  145,    9],\n",
              "          [   5,   18,   99,  ...,   29,   50,    9],\n",
              "          [2932,  116,  144,  ...,   13,  308,  190],\n",
              "          ...,\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1]],\n",
              " \n",
              "         [[ 216,   17,   18,  ...,  878,   34,    9],\n",
              "          [   1,   10,  954,  ...,   26,  314,    9],\n",
              "          [ 247,   47,  316,  ..., 2660,  677,    9],\n",
              "          ...,\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1]],\n",
              " \n",
              "         [[   1,    1,   65,  ...,   31,  353,    9],\n",
              "          [  34,  173, 3134,  ...,   37,   31,    9],\n",
              "          [   1,    5,   18,  ...,   12, 1040, 6465],\n",
              "          ...,\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[   1,    1,    1,  ...,    5,  356,   42],\n",
              "          [   1,    1,    5,  ...,   14,  443,    9],\n",
              "          [   1,    1,    1,  ..., 2931,   31,    9],\n",
              "          ...,\n",
              "          [   1,    1,    1,  ...,   10,   39,    9],\n",
              "          [   1,    1,    1,  ...,  205,   30,    9],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1]],\n",
              " \n",
              "         [[  65,   10,    5,  ...,    5,  372,    9],\n",
              "          [   1,    1,    1,  ...,  480, 1091,   36],\n",
              "          [  15,  215,  527,  ...,  141,  464,    9],\n",
              "          ...,\n",
              "          [   1,    1,    1,  ...,   43,    0,    9],\n",
              "          [ 129,   23,  551,  ...,   16,  590,    9],\n",
              "          [   1,    1,    1,  ...,   12, 1888,    9]],\n",
              " \n",
              "         [[   5,   12,    5,  ..., 1188,  828,    9],\n",
              "          [   1,    5,  866,  ...,  359,   21,    9],\n",
              "          [  26,   10,  373,  ...,    0, 1205,    9],\n",
              "          ...,\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1],\n",
              "          [   1,    1,    1,  ...,    1,    1,    1]]], dtype=torch.int32),\n",
              " tensor([ 8,  8,  9,  1, 11, 11, 11,  8,  8,  3,  0, 10, 12,  2,  9,  4, 12,  9,\n",
              "          2,  8, 14,  8, 14, 11, 10,  0, 11,  9, 11,  8,  3,  9,  2,  0,  9,  9,\n",
              "          9, 10,  9,  1,  1, 11,  8,  8, 15,  9,  3, 10, 11, 10, 10,  1,  9, 13,\n",
              "          8, 11, 14, 10,  9,  9,  1,  8,  9, 10])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heW_mAQKLNye"
      },
      "source": [
        "model = HierAttnNet(\n",
        "    vocab_size=len(vocab.stoi),\n",
        "    maxlen_sent=maxlen_sent,\n",
        "    maxlen_doc=maxlen_doc,\n",
        "    word_hidden_dim=32,\n",
        "    sent_hidden_dim=32,\n",
        "    padding_idx=1,\n",
        "    embed_dim=50,\n",
        "    # weight_drop=0.,\n",
        "    # embed_drop=0.,\n",
        "    # locked_drop=0.,\n",
        "    # last_drop=0.,\n",
        "    embedding_matrix=None,\n",
        "    num_class=len(cleaned_data['category'].unique()),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTOgJ6gFLV-i",
        "outputId": "7fb243af-073b-4606-9595-d0ccf3e3103e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HierAttnNet(\n",
              "  (wordattnnet): WordAttnNet(\n",
              "    (word_embed): Embedding(70082, 50, padding_idx=1)\n",
              "    (rnn): GRU(50, 32, batch_first=True, bidirectional=True)\n",
              "    (word_attn): AttentionWithContext(\n",
              "      (attn): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (contx): Linear(in_features=64, out_features=1, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (sentattnnet): SentAttnNet(\n",
              "    (rnn): GRU(64, 32, batch_first=True, bidirectional=True)\n",
              "    (sent_attn): AttentionWithContext(\n",
              "      (attn): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (contx): Linear(in_features=64, out_features=1, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=64, out_features=16, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHBWp3B-LgOU"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf05uZrELeW6"
      },
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "# This class is at the utils module\n",
        "metric = CategoricalAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbM6EWx8LiWO"
      },
      "source": [
        "def train_step(model, optimizer, train_loader, epoch, metric):\n",
        "    model.train()\n",
        "    metric.reset()\n",
        "    train_steps = len(train_loader)\n",
        "    running_loss = 0\n",
        "    with trange(train_steps) as t:\n",
        "        for batch_idx, (data, target) in zip(t, train_loader):\n",
        "            t.set_description(\"epoch %i\" % (epoch + 1))\n",
        "\n",
        "            X = data.cuda() if use_cuda else data\n",
        "            y = target.cuda() if use_cuda else target\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X)\n",
        "            loss = F.cross_entropy(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            avg_loss = running_loss / (batch_idx + 1)\n",
        "            acc = metric(F.softmax(y_pred, dim=1), y)\n",
        "\n",
        "            t.set_postfix(acc=acc, loss=avg_loss)\n",
        "\n",
        "\n",
        "def eval_step(model, eval_loader, metric, is_test=False):\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "    eval_steps = len(eval_loader)\n",
        "    running_loss = 0\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        with trange(eval_steps) as t:\n",
        "            for batch_idx, (data, target) in zip(t, eval_loader):\n",
        "                if is_test:\n",
        "                    t.set_description(\"test\")\n",
        "                else:\n",
        "                    t.set_description(\"valid\")\n",
        "\n",
        "                X = data.cuda() if use_cuda else data\n",
        "                y = target.cuda() if use_cuda else target\n",
        "\n",
        "                y_pred = model(X)\n",
        "                loss = F.cross_entropy(y_pred, y)\n",
        "                running_loss += loss.item()\n",
        "                avg_loss = running_loss / (batch_idx + 1)\n",
        "                acc = metric(F.softmax(y_pred, dim=1), y)\n",
        "                if is_test:\n",
        "                    preds.append(y_pred)\n",
        "                t.set_postfix(acc=acc, loss=avg_loss)\n",
        "\n",
        "    return avg_loss, preds\n",
        "\n",
        "\n",
        "def early_stopping(curr_value, best_value, stop_step, patience):\n",
        "    if curr_value <= best_value:\n",
        "        stop_step, best_value = 0, curr_value\n",
        "    else:\n",
        "        stop_step += 1\n",
        "    if stop_step >= patience:\n",
        "        print(\"Early stopping triggered. log:{}\".format(best_value))\n",
        "        stop = True\n",
        "    else:\n",
        "        stop = False\n",
        "    return best_value, stop_step, stop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNZ6jdcmLjti",
        "outputId": "b49af27a-8c46-4829-a23b-bb6428372968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "metric = CategoricalAccuracy()\n",
        "n_epochs = 23\n",
        "eval_every = 1\n",
        "patience = 3 # 1\n",
        "stop_step = 0\n",
        "best_loss = 1e6\n",
        "for epoch in range(n_epochs):\n",
        "    train_step(model, optimizer, train_loader, epoch, metric)\n",
        "    if epoch % eval_every == (eval_every - 1):\n",
        "        val_loss, _ = eval_step(model, eval_loader, metric)\n",
        "        best_loss, stop_step, stop = early_stopping(\n",
        "            val_loss, best_loss, stop_step, patience\n",
        "        )\n",
        "    if stop:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1: 100%|██████████| 109/109 [04:03<00:00,  2.24s/it, acc=0.201, loss=2.35]\n",
            "valid: 100%|██████████| 14/14 [00:05<00:00,  2.53it/s, acc=0.211, loss=2.28]\n",
            "epoch 2: 100%|██████████| 109/109 [04:07<00:00,  2.27s/it, acc=0.215, loss=2.28]\n",
            "valid: 100%|██████████| 14/14 [00:05<00:00,  2.49it/s, acc=0.21, loss=2.26]\n",
            "epoch 3: 100%|██████████| 109/109 [04:09<00:00,  2.29s/it, acc=0.3, loss=2.13]\n",
            "valid: 100%|██████████| 14/14 [00:05<00:00,  2.50it/s, acc=0.396, loss=1.94]\n",
            "epoch 4: 100%|██████████| 109/109 [04:05<00:00,  2.26s/it, acc=0.508, loss=1.71]\n",
            "valid: 100%|██████████| 14/14 [00:05<00:00,  2.56it/s, acc=0.529, loss=1.62]\n",
            "epoch 5: 100%|██████████| 109/109 [04:08<00:00,  2.28s/it, acc=0.58, loss=1.47]\n",
            "valid: 100%|██████████| 14/14 [00:06<00:00,  2.29it/s, acc=0.572, loss=1.53]\n",
            "epoch 6: 100%|██████████| 109/109 [04:05<00:00,  2.25s/it, acc=0.622, loss=1.32]\n",
            "valid: 100%|██████████| 14/14 [00:05<00:00,  2.54it/s, acc=0.583, loss=1.5]\n",
            "epoch 7: 100%|██████████| 109/109 [04:06<00:00,  2.27s/it, acc=0.656, loss=1.2]\n",
            "valid: 100%|██████████| 14/14 [00:05<00:00,  2.49it/s, acc=0.58, loss=1.52]\n",
            "epoch 8: 100%|██████████| 109/109 [04:07<00:00,  2.27s/it, acc=0.692, loss=1.08]\n",
            "valid: 100%|██████████| 14/14 [00:05<00:00,  2.51it/s, acc=0.576, loss=1.56]\n",
            "epoch 9: 100%|██████████| 109/109 [04:09<00:00,  2.29s/it, acc=0.725, loss=0.977]\n",
            "valid: 100%|██████████| 14/14 [00:05<00:00,  2.51it/s, acc=0.561, loss=1.65]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Early stopping triggered. log:1.4968211821147375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9fKQvB8g_PY"
      },
      "source": [
        "# Running on custom inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX0EPAjGhBbB"
      },
      "source": [
        "def find_personality(sentence, clean=False):\n",
        "  # clean the sentence if required\n",
        "  if clean:\n",
        "    sentence = sentence.strip()\n",
        "    # Step 1: Remove links\n",
        "    sentence = re.sub(r'http\\S+', '', sentence)\n",
        "    # Step 2: Remove multiple full stops\n",
        "    sentence = re.sub(r'\\.+', \".\", sentence)\n",
        "    # Step 3: Some sentences do not have space after full stop, remove that\n",
        "    sentence = sentence.replace(\".\", \". \")\n",
        "    # Step 4: remove multiple spaces\n",
        "    sentence = re.sub(' +', ' ', sentence)\n",
        "    # Step 5: remove punctuations except . , ! & ? '\n",
        "    sentence = re.sub(\"[^a-zA-Z.,!&?']\", \" \", sentence)\n",
        "  \n",
        "  # convert sentence into list of sentence\n",
        "  sentences = [sentence]\n",
        "\n",
        "\n",
        "  tok_func = spacy.load(\"en_core_web_sm\")\n",
        "  n_cpus = os.cpu_count()\n",
        "  texts_sents = []\n",
        "  for doc in tok_func.pipe(sentences, n_process=n_cpus):\n",
        "      sents = [str(s) for s in list(doc.sents)]\n",
        "      texts_sents.append(sents)\n",
        "\n",
        "  all_sents = [s for sents in texts_sents for s in sents]\n",
        "  texts_length = [0] + [len(s) for s in texts_sents]\n",
        "  range_idx = [sum(texts_length[: i + 1]) for i in range(len(texts_length))]\n",
        "\n",
        "  sents_tokens = Tokenizer().process_all(all_sents)\n",
        "\n",
        "  sents_tokens_2 = get_texts(all_sents, with_preprocess=True)\n",
        "\n",
        "  new_tok = Tokenizer()\n",
        "  new_tok.pre_rules = [rm_punctuation] + new_tok.pre_rules  \n",
        "\n",
        "  sents_numz = [vocab.numericalize(s) for s in sents_tokens]\n",
        "\n",
        "  texts_numz = [sents_numz[range_idx[i] : range_idx[i + 1]] for i in range(len(range_idx[:-1]))]\n",
        "\n",
        "  padded_texts = np.stack([pad_nested_sequences(r, maxlen_sent, maxlen_doc) for r in texts_numz], axis=0)\n",
        "\n",
        "  custom_data = torch.from_numpy(np.array(padded_texts))\n",
        "\n",
        "  X = custom_data.cuda() if use_cuda else custom_data\n",
        "  y_pred = model(X)\n",
        "  # indx = torch.max(y_pred[0])\n",
        "  idx2 = int(torch.argmax(y_pred[0]).cpu().numpy())\n",
        "  sent_category = categories[idx2]\n",
        "  return sent_category\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOhTVdcTq-8k",
        "outputId": "b07157d7-2cd6-4a51-a134-23df0f5c5061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "find_personality(\"If you're visiting this page, you're likely here because you're searching for a random sentence. Sometimes a random word just isn't enough, and that is where the random sentence generator comes into play. By inputting the desired number, you can make a list of as many random sentences as you want or need. Producing random sentences can be helpful in a number of different ways.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'INTP'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMJzxbZpwKOq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}